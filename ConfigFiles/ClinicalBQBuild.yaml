#
# YAML Configuration file used for script build_clinical_data_bq_table.py
# Builds BQ table for clinical data.
#

version: 3

api_params:
  # GDC API endpoint for retrieval of cases dataset
  ENDPOINT: https://api.gdc.cancer.gov/cases
  FILES_ENDPOINT: https://api.gdc.cancer.gov/files

  # List of 'expand' field groups to include in the clinical data bq table
  FIELD_GROUPS:
    - demographic
    - diagnoses
    - diagnoses.treatments
    - diagnoses.annotations
    - exposures
    - family_histories
    - follow_ups
    - follow_ups.molecular_tests
    - project
    - files
    - samples
    - samples.portions
    - samples.portions.analytes
    - samples.portions.analytes.aliquots
    - samples.portions.slides

  # fields that aren't desired for bq
  EXCLUDE_FIELDS:
    aliquot_ids: 'exclude' 
    analyte_ids: 'exclude'
    case_autocomplete: 'exclude'
    diagnosis_ids: 'exclude'
    id: 'exclude'
    portion_ids: 'exclude'
    slide_ids: 'exclude'
    submitter_aliquot_ids: 'exclude'
    submitter_analyte_ids: 'exclude'
    submitter_diagnosis_ids: 'exclude'
    submitter_portion_ids: 'exclude'
    submitter_slide_ids: 'exclude'
    files:
      proportion_coverage_10x: 'exclude'
      proportion_coverage_30x: 'exclude'
      proportion_coverage_10X: 'exclude'
      proportion_coverage_30X: 'exclude'
  
  TRANSFORM_FIELDS:  
    samples:
      portions:
          analytes:
              aliquots:
                  source_center: 
                      transformations: 
                          simple: [['none_to_zero','']]
                           # usually integer (22,23) sometimes 'None'

  # How many case records to retrieve per GDC API call. Larger batch sizes are more
  # likely to fail before completion, seems to work consistently at 2500
  BATCH_SIZE: 2500 #2500

  # Start index for retrieving case records
  START_INDEX: 0

  # Number of pages to write into json file (0 == all pages after start index)
  MAX_PAGES: 0


bq_params:
  ##
  #  File Locations, GDC Release, Naming Conventions
  ##

  # Directory to which to write the cases clinical data json file
  SCRATCH_DIR: scratch

  # File to which to write the cases clinical data json file
  DATA_OUTPUT_FILE: clinical_file_data_v1.jsonl

  # 'a' if appending to existing cases json (for continuation of interrupted file build).
  # 'w' if creating or overwriting existing CASES_JSON_FILE.
  IO_MODE: 'w'

  # What bucket is going to get the text file heading to BQ?
  WORKING_BUCKET: gdc-bq-sample-bucket

  # What is the file path to the text file in the bucket:
  WORKING_BUCKET_DIR: druth

  # name for master table (will be prefixed with GDC_RELEASE value)
  MASTER_TABLE: clinical_and_file

  # in case we switch back to relXX from rXX
  REL_PREFIX: 'r'

  # most recent GDC release number
  # (NOTE: pulls data from the current release regardless of value here, not currently
  # possible to specify a release number when making API calls.)
  RELEASE: '26'

  LOCATION: US

  ##
  #  BigQuery API
  ##

  # What project are we in:
  DEV_PROJECT: gdc-bq-sample

  # Where is the BQ table dataset:
  DEV_DATASET: gdc_metadata

  # Note that although the steps are given in the actual order here as
  # a list, changing the order here does not change the order of execution, which is fixed.

steps:
  # Get the manifest from the source data node:
  - retrieve_cases_and_write_to_jsonl
  
  - transform_data_in_jsonl

  - upload_jsonl_to_cloud_storage

  # Get the table schema/description/tags pulled from git:
  - create_bq_schema_obj

  # Build BQ Table
  - build_bq_table
